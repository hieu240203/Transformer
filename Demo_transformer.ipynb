{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1717236247752,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"T8sNba4vXz6B"},"outputs":[],"source":["import codecs\n","import csv\n","import re\n","import sys\n","import pickle"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10795,"status":"ok","timestamp":1717236258540,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"QevskijJX1Fo"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1717236258541,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"xtiKgEs1bPs8"},"outputs":[],"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1717236258542,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"vtSxDs8AX4BX"},"outputs":[],"source":["def encode(ipt, opt):\n","  ipt = [tokenizer_ipt.vocab_size] + tokenizer_ipt.encode(\n","      ipt.numpy()) + [tokenizer_ipt.vocab_size+1]\n","\n","  opt = [tokenizer_opt.vocab_size] + tokenizer_opt.encode(\n","      opt.numpy()) + [tokenizer_opt.vocab_size+1]\n","\n","  return ipt, opt"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1717236258542,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"ZxheUrw5Xpaa"},"outputs":[],"source":["def tf_encode(ipt, opt):\n","  result_ipt, result_opt = tf.py_function(encode, [ipt, opt], [tf.int64, tf.int64])\n","  result_ipt.set_shape([None])\n","  result_opt.set_shape([None])\n","  return result_ipt, result_opt"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1717236258542,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"yzEUdD2xX2Ed"},"outputs":[],"source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1717236258542,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"EJxjInMOX8UY"},"outputs":[],"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model) # shape (position, d_model)\n","\n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32) # shape: (position, d_model)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1717236258543,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"R5bemyh0X-Gu"},"outputs":[],"source":["def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1717236258931,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"syAFObNoX_SJ"},"outputs":[],"source":["def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1717236258931,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"gDfU1H2FYA5n"},"outputs":[],"source":["def scaled_dot_product_attention(q, k, v, mask):\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32) # depth\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)\n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1717236258931,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"iRWeZQD5YCjA"},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","\n","  def split_heads(self, x, batch_size):\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","    return output, attention_weights"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1717236258931,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"Fr1bxsr0YEBQ"},"outputs":[],"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1717236258932,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"ZRwuYPiPYFMY"},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","    return out2"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1717236258932,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"4dbNuh7mYGY4"},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","\n","  def call(self, x, enc_output, training,\n","           look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = self.layernorm1(attn1 + x)\n","\n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","    return out3, attn_weights_block1, attn_weights_block2"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1717236258932,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"FIcgzRE5YHo-"},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                            self.d_model)\n","\n","\n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n","                       for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","\n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x  # (batch_size, input_seq_len, d_model)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5184,"status":"ok","timestamp":1717236264106,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"Wx8mmFy6ZZdL","outputId":"457335ce-6027-4bd1-e48e-8c8c141dcb0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 62, 512)\n"]}],"source":["sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n","                         dff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","\n","# Init sample tensorflow with shape 64 x 62 and data type int.\n","temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n","\n","sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n","\n","print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1717236264106,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"DxDyQdAhZMZd"},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n","                       for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, enc_output, training,\n","           look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","\n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1717236264106,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"vBXzBq04YJDX"},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                            self.d_model)\n","\n","\n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n","                       for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","\n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x  # (batch_size, input_seq_len, d_model)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1717236264106,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"A0ADA4eUYKZ2"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n","                           input_vocab_size, pe_input, rate)\n","\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n","                           target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inp, tar, training, enc_padding_mask,\n","           look_ahead_mask, dec_padding_mask):\n","    # print('enc_padding_mask: ', enc_padding_mask)\n","    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    dec_output, attention_weights = self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","    return final_output, attention_weights"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1717236264106,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"YeqC_jTlaOvW"},"outputs":[],"source":["\n","def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","\n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by\n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  return enc_padding_mask, combined_mask, dec_padding_mask"]},{"cell_type":"markdown","metadata":{"id":"1dmljCH0d3rC"},"source":["# Pretrain"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":1956,"status":"ok","timestamp":1717236266043,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"GpY0b6RqYYD0"},"outputs":[],"source":["def _save_pickle(path, obj):\n","  with open(path, 'wb') as f:\n","    pickle.dump(obj, f)\n","\n","def _load_pickle(path):\n","  with open(path, 'rb') as f:\n","    obj = pickle.load(f)\n","  return obj\n","\n","tokenizer_ipt = _load_pickle('/content/drive/MyDrive/Tự học/NLP/NLP-CNWEB/Nhóm 1/tokenizer_ipt.pkl')\n","tokenizer_opt = _load_pickle('/content/drive/MyDrive/Tự học/NLP/NLP-CNWEB/Nhóm 1/tokenizer_opt.pkl')\n","\n","# Khai báo tham số\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_ipt.vocab_size + 2\n","target_vocab_size = tokenizer_opt.vocab_size + 2\n","dropout_rate = 0.1\n","learning_rate = 0.01\n","MAX_LENGTH = 40"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3120,"status":"ok","timestamp":1717236269159,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"RMMGdkTWYapX"},"outputs":[],"source":["transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n","                          pe_input=input_vocab_size,\n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)\n","\n","checkpoint_path = \"/content/drive/MyDrive/Tự học/NLP/NLP-CNWEB/checkPoint\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1717236269159,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"uWrja-wXZSFU"},"outputs":[],"source":["MAX_LENGTH = 40\n","\n","def evaluate(inp_sentence):\n","  start_token = [tokenizer_ipt.vocab_size]\n","  end_token = [tokenizer_ipt.vocab_size + 1]\n","\n","  # inp sentence is non_diacritic, hence adding the start and end token\n","  inp_sentence = start_token + tokenizer_ipt.encode(inp_sentence) + end_token\n","  encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","  # as the target is exist diacritic, the first word to the transformer should be the\n","  # english start token.\n","  decoder_input = [tokenizer_opt.vocab_size]\n","  output = tf.expand_dims(decoder_input, 0)\n","\n","  for i in range(MAX_LENGTH):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","\n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input,\n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","\n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # return the result if the predicted_id is equal to the end token\n","    if predicted_id == tokenizer_opt.vocab_size+1:\n","      return tf.squeeze(output, axis=0), attention_weights\n","\n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0), attention_weights"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1717236269159,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"},"user_tz":-420},"id":"2Yngcv9PZp0U"},"outputs":[],"source":["def translate(sentence, plot=''):\n","  result, attention_weights = evaluate(sentence)\n","\n","  predicted_sentence = tokenizer_opt.decode([i for i in result\n","                                            if i < tokenizer_opt.vocab_size])\n","\n","  print('Input: {}'.format(sentence))\n","  print('Predicted translation: {}'.format(predicted_sentence))\n","\n","  # if plot:\n","  #   plot_attention_weights(attention_weights, sentence, result, plot)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bV9oQU4veDu8","outputId":"ac82c219-8626-43c4-9a1e-6d393a3740ae","executionInfo":{"status":"ok","timestamp":1717236289324,"user_tz":-420,"elapsed":20174,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fastapi\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.30.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyngrok\n","  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.11.0)\n","Collecting fastapi-cli>=0.0.2 (from fastapi)\n","  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n","Collecting httpx>=0.23.0 (from fastapi)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n","Collecting python-multipart>=0.0.7 (from fastapi)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi)\n","  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson>=3.2.1 (from fastapi)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n","Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.23.0->fastapi)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n","Collecting httptools>=0.5.0 (from uvicorn)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn)\n","  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n","Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n","Installing collected packages: websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, pyngrok, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, fastapi-cli, fastapi\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pyngrok-7.1.6 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.0 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"]}],"source":["!pip install fastapi uvicorn pyngrok"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"tKV59C4KlRO8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717236290301,"user_tz":-420,"elapsed":985,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"}},"outputId":"5c1e22d8-90d4-43e6-eb76-971d48ee1333"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-26-fae8f69bd4f6>:128: DeprecationWarning: \n","        on_event is deprecated, use lifespan event handlers instead.\n","\n","        Read more about it in the\n","        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n","        \n","  @app.on_event(\"startup\")\n"]}],"source":["from fastapi import FastAPI, HTTPException\n","from fastapi.responses import HTMLResponse\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel\n","from pyngrok import ngrok, conf\n","import nest_asyncio\n","import uvicorn\n","\n","\n","\n","# Định nghĩa ứng dụng FastAPI\n","app = FastAPI()\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=['*'],\n","    allow_credentials=True,\n","    allow_methods=['*'],\n","    allow_headers=['*'],\n",")\n","\n","index = \"\"\"<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title>Thêm dấu tiếng Việt</title>\n","    <style>\n","        body {\n","            font-family: Arial, sans-serif;\n","            background-color: #f4f4f4;\n","            display: flex;\n","            justify-content: center;\n","            align-items: center;\n","            height: 100vh;\n","            margin: 0;\n","        }\n","        .container {\n","            background-color: #fff;\n","            padding: 20px;\n","            border-radius: 8px;\n","            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n","            max-width: 500px;\n","            width: 100%;\n","        }\n","        h1 {\n","            text-align: center;\n","            color: #333;\n","        }\n","        label {\n","            font-weight: bold;\n","            color: #555;\n","        }\n","        textarea {\n","            width: 100%;\n","            height: 100px;\n","            padding: 10px;\n","            margin-top: 10px;\n","            border: 1px solid #ddd;\n","            border-radius: 4px;\n","            font-size: 16px;\n","        }\n","        button {\n","            width: 100%;\n","            padding: 10px;\n","            margin-top: 20px;\n","            background-color: #007BFF;\n","            color: white;\n","            border: none;\n","            border-radius: 4px;\n","            font-size: 16px;\n","            cursor: pointer;\n","        }\n","        button:hover {\n","            background-color: #0056b3;\n","        }\n","        h3 {\n","            margin-top: 20px;\n","            color: #333;\n","        }\n","        p {\n","            background-color: #f4f4f4;\n","            padding: 10px;\n","            border-radius: 4px;\n","            border: 1px solid #ddd;\n","            color: #333;\n","        }\n","    </style>\n","</head>\n","<body>\n","    <div class=\"container\">\n","        <h1>Translator</h1>\n","        <form action=\"\" onsubmit=\"sendMessage(event)\">\n","            <label for=\"originalText\"><b>Nhập từ chưa thêm dấu </b></label>\n","            <br><br>\n","            <textarea id=\"originalText\" name=\"originalText\" autocomplete=\"off\"></textarea>\n","            <br>\n","            <button>Translate</button>\n","        </form>\n","        <h3>Đã thêm dấu</h3>\n","        <p id=\"translatedText\"></p>\n","    </div>\n","    <script>\n","        function sendMessage(event) {\n","            var text = document.getElementById('originalText');\n","            var xhr = new XMLHttpRequest();\n","            xhr.open(\"POST\", '/translate', true);\n","            xhr.onreadystatechange = function() {\n","                if (this.readyState == 4 && this.status == 200) {\n","                    var data = JSON.parse(this.responseText);\n","                    document.getElementById('translatedText').innerText = data['text'];\n","                }\n","            }\n","            xhr.setRequestHeader('Content-Type', 'application/json');\n","            xhr.send(JSON.stringify({\n","                text: text.value\n","            }));\n","            event.preventDefault();\n","        }\n","    </script>\n","</body>\n","</html>\n","\"\"\"\n","\n","\n","translator = None\n","\n","@app.on_event(\"startup\")\n","async def startup():\n","    global translator\n","    translator = translate\n","\n","@app.get('/')\n","async def root():\n","    return HTMLResponse(index)\n","\n","class Text(BaseModel):\n","    text: str\n","\n","@app.post('/translate')\n","async def translate_text(data: Text):\n","    if not data.text:\n","        raise HTTPException(status_code=400, detail=\"Text to translate cannot be empty\")\n","\n","    # Call the translate function and capture the translation\n","    result, _ = evaluate(data.text)\n","    translated_text = tokenizer_opt.decode([i for i in result if i < tokenizer_opt.vocab_size])\n","\n","    return {'text': translated_text}\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"I4kovEZ9dTTF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717236291669,"user_tz":-420,"elapsed":1374,"user":{"displayName":"hiếu nguyễn văn","userId":"03003229465244128870"}},"outputId":"676a9e06-997d-4642-8b5b-89570109713f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!ngrok config add-authtoken 2hExKPYTmtMqVgRLDGxCzGGTTzd_67Hqo4RgTTvQyp3yEfNn6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9o_jLHKgvW_","outputId":"c29f8f7f-1c72-46d0-d892-fa2464a54ea8"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [395]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":["Public URL: https://7c9b-35-231-70-72.ngrok-free.app\n","INFO:     183.80.130.8:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     183.80.130.8:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n","INFO:     183.80.130.8:0 - \"POST /translate HTTP/1.1\" 200 OK\n","INFO:     183.80.130.8:0 - \"POST /translate HTTP/1.1\" 200 OK\n","INFO:     183.80.130.8:0 - \"POST /translate HTTP/1.1\" 200 OK\n","INFO:     183.80.130.8:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     183.80.130.8:0 - \"POST /translate HTTP/1.1\" 200 OK\n"]}],"source":["import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","\n","\n","port = 8000\n","ngrok_tunnel = ngrok.connect(port)\n","\n","print('Public URL:', ngrok_tunnel.public_url)\n","\n","\n","nest_asyncio.apply()\n","\n","uvicorn.run(app, port=port)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f6mBv1ogyKX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1GWuQ7mOz0cBaH7Uaj13lwYHC9evq_5oO","authorship_tag":"ABX9TyOPhz1PDbzkdjoZ8exBHNNV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}